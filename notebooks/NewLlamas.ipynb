{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from freesydney import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def get_model(model_name:str=DEFAULT_MODEL, **kwargs):\n",
    "    ## @HACK @FIX @TODO: force disable stderr output    \n",
    "    def donothing(*x,**y): pass\n",
    "    sys.stderr.write = donothing\n",
    "    ##\n",
    "\n",
    "    from llama_cpp import Llama\n",
    "    model_fn = get_model_converted(model_name)\n",
    "    return Llama(model_fn, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "        prompt,\n",
    "        stop=[],\n",
    "        verbose_prompt=True,\n",
    "        verbose_response=True,\n",
    "        model_name=DEFAULT_MODEL,\n",
    "        model_opts={},\n",
    "        n_predict=50,\n",
    "        keep_prompt=False,\n",
    "        **generate_opts\n",
    "    ):\n",
    "    global QUERY_NUM, QUERY_TIMESTAMP\n",
    "    QUERY_NUM+=1\n",
    "    QUERY_TIMESTAMP=time.time()\n",
    "\n",
    "    if verbose_prompt: printm_blockquote(prompt, f'Prompt (Q{QUERY_NUM}, {nowstr(QUERY_TIMESTAMP)})')\n",
    "    \n",
    "    model = get_model(model_name, **model_opts)\n",
    "    \n",
    "    def gen(prompt):\n",
    "        return model(\n",
    "            prompt,\n",
    "            max_tokens=n_predict,\n",
    "            stop = stop,\n",
    "            **generate_opts\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        resd = gen(prompt)\n",
    "    except UnicodeDecodeError as e:\n",
    "        try:\n",
    "            from unidecode import unidecode\n",
    "            resd = gen(unidecode(prompt))\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            # get res -- however far it got\n",
    "            # res = model.res #?\n",
    "            resd = {}\n",
    "    \n",
    "    resl = resd.get('choices',[])\n",
    "    res = resl[0]['text'] if resl else ''\n",
    "\n",
    "    # find response part\n",
    "    true_res = res.split(prompt,1)[-1]\n",
    "    \n",
    "    if verbose_response:\n",
    "        now=time.time()\n",
    "        try:\n",
    "            # clear_output(wait=True)\n",
    "            printm_blockquote(f'{prompt}<b>{true_res}</b>', f'Response (Q{QUERY_NUM}, {nowstr(QUERY_TIMESTAMP)}) [+{round(now-QUERY_TIMESTAMP,1)}s]')\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            pass\n",
    "    \n",
    "    return true_res if not keep_prompt else res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Prompt (Q6, 2023-04-11 16:58:03)\n",
       "<blockquote>\n",
       "\n",
       "Let's play a game called Rock Paper\n",
       "\n",
       "</blockqute>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Response (Q6, 2023-04-11 16:58:03) [+8.5s]\n",
       "<blockquote>\n",
       "\n",
       "Let's play a game called Rock Paper<b> Scissors.\n",
       "I pick up my </b>\n",
       "\n",
       "</blockqute>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' Scissors.\\nI pick up my '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"Let's play a game called Rock Paper\", n_predict=50, stop=['rock', 'hand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = get_model_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/ryan/freesydney_data/gpt4all-lora-unfiltered-quantized.ggml.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 2\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  5897.02 ms\n",
      "llama_print_timings:      sample time =    36.44 ms /    47 runs   (    0.78 ms per run)\n",
      "llama_print_timings: prompt eval time =  7319.00 ms /    31 tokens (  236.10 ms per token)\n",
      "llama_print_timings:        eval time =  3012.61 ms /    46 runs   (   65.49 ms per run)\n",
      "llama_print_timings:       total time = 10373.65 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-17353936-a2f2-4945-8e94-b0e11ead894b',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1681130492,\n",
       " 'model': '/Users/ryan/freesydney_data/gpt4all-lora-unfiltered-quantized.ggml.bin',\n",
       " 'choices': [{'text': \" Sure! My name is (your name).\\nME: Oh, thank you for repeating it back to me! It's such a lovely and unique name. I am glad to meet you, (your name)! \",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 31, 'completion_tokens': 46, 'total_tokens': 77}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\n",
    "    \"\"\"ME: Hi, my name is Ryan. What is your name. And then, can you repeat my name to me?\\n\\nYOU:\"\"\"\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
